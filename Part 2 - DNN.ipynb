{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:05:19.857859Z",
     "start_time": "2019-05-14T03:05:19.706364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:05:20.434588Z",
     "start_time": "2019-05-14T03:05:20.395230Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:04:41.528600Z",
     "start_time": "2019-05-14T03:04:37.188060Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\tCost: 0.708491325378418\n",
      "Step: 100\tCost: 0.6931474804878235\n",
      "Step: 200\tCost: 0.6931471824645996\n",
      "Step: 300\tCost: 0.6931471824645996\n",
      "Step: 400\tCost: 0.6931471824645996\n",
      "Step: 500\tCost: 0.6931471824645996\n",
      "Step: 600\tCost: 0.6931471824645996\n",
      "Step: 700\tCost: 0.6931471824645996\n",
      "Step: 800\tCost: 0.6931471824645996\n",
      "Step: 900\tCost: 0.6931471824645996\n",
      "Step: 1000\tCost: 0.6931471824645996\n",
      "Step: 1100\tCost: 0.6931471824645996\n",
      "Step: 1200\tCost: 0.6931471824645996\n",
      "Step: 1300\tCost: 0.6931471824645996\n",
      "Step: 1400\tCost: 0.6931471824645996\n",
      "Step: 1500\tCost: 0.6931471824645996\n",
      "Step: 1600\tCost: 0.6931471824645996\n",
      "Step: 1700\tCost: 0.6931471824645996\n",
      "Step: 1800\tCost: 0.6931471824645996\n",
      "Step: 1900\tCost: 0.6931471824645996\n",
      "Step: 2000\tCost: 0.6931471824645996\n",
      "Step: 2100\tCost: 0.6931471824645996\n",
      "Step: 2200\tCost: 0.6931471824645996\n",
      "Step: 2300\tCost: 0.6931471824645996\n",
      "Step: 2400\tCost: 0.6931471824645996\n",
      "Step: 2500\tCost: 0.6931471824645996\n",
      "Step: 2600\tCost: 0.6931471824645996\n",
      "Step: 2700\tCost: 0.6931471824645996\n",
      "Step: 2800\tCost: 0.6931471824645996\n",
      "Step: 2900\tCost: 0.6931471824645996\n",
      "Step: 3000\tCost: 0.6931471824645996\n",
      "Step: 3100\tCost: 0.6931471824645996\n",
      "Step: 3200\tCost: 0.6931471824645996\n",
      "Step: 3300\tCost: 0.6931471824645996\n",
      "Step: 3400\tCost: 0.6931471824645996\n",
      "Step: 3500\tCost: 0.6931471824645996\n",
      "Step: 3600\tCost: 0.6931471824645996\n",
      "Step: 3700\tCost: 0.6931471824645996\n",
      "Step: 3800\tCost: 0.6931471824645996\n",
      "Step: 3900\tCost: 0.6931471824645996\n",
      "Step: 4000\tCost: 0.6931471824645996\n",
      "Step: 4100\tCost: 0.6931471824645996\n",
      "Step: 4200\tCost: 0.6931471824645996\n",
      "Step: 4300\tCost: 0.6931471824645996\n",
      "Step: 4400\tCost: 0.6931471824645996\n",
      "Step: 4500\tCost: 0.6931471824645996\n",
      "Step: 4600\tCost: 0.6931471824645996\n",
      "Step: 4700\tCost: 0.6931471824645996\n",
      "Step: 4800\tCost: 0.6931471824645996\n",
      "Step: 4900\tCost: 0.6931471824645996\n",
      "Step: 5000\tCost: 0.6931471824645996\n",
      "Step: 5100\tCost: 0.6931471824645996\n",
      "Step: 5200\tCost: 0.6931471824645996\n",
      "Step: 5300\tCost: 0.6931471824645996\n",
      "Step: 5400\tCost: 0.6931471824645996\n",
      "Step: 5500\tCost: 0.6931471824645996\n",
      "Step: 5600\tCost: 0.6931471824645996\n",
      "Step: 5700\tCost: 0.6931471824645996\n",
      "Step: 5800\tCost: 0.6931471824645996\n",
      "Step: 5900\tCost: 0.6931471824645996\n",
      "Step: 6000\tCost: 0.6931471824645996\n",
      "Step: 6100\tCost: 0.6931471824645996\n",
      "Step: 6200\tCost: 0.6931471824645996\n",
      "Step: 6300\tCost: 0.6931471824645996\n",
      "Step: 6400\tCost: 0.6931471824645996\n",
      "Step: 6500\tCost: 0.6931471824645996\n",
      "Step: 6600\tCost: 0.6931471824645996\n",
      "Step: 6700\tCost: 0.6931471824645996\n",
      "Step: 6800\tCost: 0.6931471824645996\n",
      "Step: 6900\tCost: 0.6931471824645996\n",
      "Step: 7000\tCost: 0.6931471824645996\n",
      "Step: 7100\tCost: 0.6931471824645996\n",
      "Step: 7200\tCost: 0.6931471824645996\n",
      "Step: 7300\tCost: 0.6931471824645996\n",
      "Step: 7400\tCost: 0.6931471824645996\n",
      "Step: 7500\tCost: 0.6931471824645996\n",
      "Step: 7600\tCost: 0.6931471824645996\n",
      "Step: 7700\tCost: 0.6931471824645996\n",
      "Step: 7800\tCost: 0.6931471824645996\n",
      "Step: 7900\tCost: 0.6931471824645996\n",
      "Step: 8000\tCost: 0.6931471824645996\n",
      "Step: 8100\tCost: 0.6931471824645996\n",
      "Step: 8200\tCost: 0.6931471824645996\n",
      "Step: 8300\tCost: 0.6931471824645996\n",
      "Step: 8400\tCost: 0.6931471824645996\n",
      "Step: 8500\tCost: 0.6931471824645996\n",
      "Step: 8600\tCost: 0.6931471824645996\n",
      "Step: 8700\tCost: 0.6931471824645996\n",
      "Step: 8800\tCost: 0.6931471824645996\n",
      "Step: 8900\tCost: 0.6931471824645996\n",
      "Step: 9000\tCost: 0.6931471824645996\n",
      "Step: 9100\tCost: 0.6931471824645996\n",
      "Step: 9200\tCost: 0.6931471824645996\n",
      "Step: 9300\tCost: 0.6931471824645996\n",
      "Step: 9400\tCost: 0.6931471824645996\n",
      "Step: 9500\tCost: 0.6931471824645996\n",
      "Step: 9600\tCost: 0.6931471824645996\n",
      "Step: 9700\tCost: 0.6931471824645996\n",
      "Step: 9800\tCost: 0.6931471824645996\n",
      "Step: 9900\tCost: 0.6931471824645996\n",
      "Step: 10000\tCost: 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# nn layers\n",
    "linear = torch.nn.Linear(2,1, bias = True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear, sigmoid).to(device)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    \n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(\"Step: {}\\tCost: {}\".format(step, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:04:41.534395Z",
     "start_time": "2019-05-14T03:04:41.531048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTube Link: [LINK](https://www.youtube.com/watch?v=573EZkzfnZ0&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=27) (Season 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:13:59.330294Z",
     "start_time": "2019-05-14T03:13:59.321916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code Implementation\n",
    "\n",
    "# Data\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# nn layers\n",
    "w1 = torch.Tensor(2, 2).to(device)\n",
    "b1 = torch.Tensor(2).to(device)\n",
    "w2 = torch.Tensor(2, 1).to(device)\n",
    "b2 = torch.Tensor(1).to(device)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Sigmoid function\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "    # return torch.div(torch.tensor(1), torch.add(torch.tensor(1.0), torch.exp(-x)))\n",
    "    \n",
    "def sigmoid_prime(x):\n",
    "    # Derivative of the sigmoid function\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:13:59.455858Z",
     "start_time": "2019-05-14T03:13:59.453369Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 3e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:14:06.301927Z",
     "start_time": "2019-05-14T03:13:59.623531Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 \t Cost: 0.6931471824645996\n",
      "Step: 100 \t Cost: 0.6931471824645996\n",
      "Step: 200 \t Cost: 0.6931471824645996\n",
      "Step: 300 \t Cost: 0.6931471824645996\n",
      "Step: 400 \t Cost: 0.6931471824645996\n",
      "Step: 500 \t Cost: 0.6931471824645996\n",
      "Step: 600 \t Cost: 0.6931471824645996\n",
      "Step: 700 \t Cost: 0.6931471824645996\n",
      "Step: 800 \t Cost: 0.6931471824645996\n",
      "Step: 900 \t Cost: 0.6931471824645996\n",
      "Step: 1000 \t Cost: 0.6931471824645996\n",
      "Step: 1100 \t Cost: 0.6931471824645996\n",
      "Step: 1200 \t Cost: 0.6931471824645996\n",
      "Step: 1300 \t Cost: 0.6931471824645996\n",
      "Step: 1400 \t Cost: 0.6931471824645996\n",
      "Step: 1500 \t Cost: 0.6931471824645996\n",
      "Step: 1600 \t Cost: 0.6931471824645996\n",
      "Step: 1700 \t Cost: 0.6931471824645996\n",
      "Step: 1800 \t Cost: 0.6931471824645996\n",
      "Step: 1900 \t Cost: 0.6931471824645996\n",
      "Step: 2000 \t Cost: 0.6931471824645996\n",
      "Step: 2100 \t Cost: 0.6931471824645996\n",
      "Step: 2200 \t Cost: 0.6931471824645996\n",
      "Step: 2300 \t Cost: 0.6931471824645996\n",
      "Step: 2400 \t Cost: 0.6931471824645996\n",
      "Step: 2500 \t Cost: 0.6931471228599548\n",
      "Step: 2600 \t Cost: 0.6931470632553101\n",
      "Step: 2700 \t Cost: 0.6931470632553101\n",
      "Step: 2800 \t Cost: 0.6931468844413757\n",
      "Step: 2900 \t Cost: 0.6931466460227966\n",
      "Step: 3000 \t Cost: 0.693146288394928\n",
      "Step: 3100 \t Cost: 0.6931455135345459\n",
      "Step: 3200 \t Cost: 0.6931443214416504\n",
      "Step: 3300 \t Cost: 0.6931421756744385\n",
      "Step: 3400 \t Cost: 0.6931384801864624\n",
      "Step: 3500 \t Cost: 0.6931319236755371\n",
      "Step: 3600 \t Cost: 0.6931205987930298\n",
      "Step: 3700 \t Cost: 0.6931008100509644\n",
      "Step: 3800 \t Cost: 0.6930664777755737\n",
      "Step: 3900 \t Cost: 0.6930073499679565\n",
      "Step: 4000 \t Cost: 0.6929059028625488\n",
      "Step: 4100 \t Cost: 0.6927333474159241\n",
      "Step: 4200 \t Cost: 0.6924436688423157\n",
      "Step: 4300 \t Cost: 0.6919656991958618\n",
      "Step: 4400 \t Cost: 0.6911958456039429\n",
      "Step: 4500 \t Cost: 0.6899965405464172\n",
      "Step: 4600 \t Cost: 0.6882093548774719\n",
      "Step: 4700 \t Cost: 0.6856896281242371\n",
      "Step: 4800 \t Cost: 0.6823557019233704\n",
      "Step: 4900 \t Cost: 0.6782192587852478\n",
      "Step: 5000 \t Cost: 0.6733623147010803\n",
      "Step: 5100 \t Cost: 0.6678680777549744\n",
      "Step: 5200 \t Cost: 0.6617578268051147\n",
      "Step: 5300 \t Cost: 0.6549700498580933\n",
      "Step: 5400 \t Cost: 0.6473786234855652\n",
      "Step: 5500 \t Cost: 0.6388235688209534\n",
      "Step: 5600 \t Cost: 0.6291371583938599\n",
      "Step: 5700 \t Cost: 0.6181581020355225\n",
      "Step: 5800 \t Cost: 0.6057409048080444\n",
      "Step: 5900 \t Cost: 0.5917649269104004\n",
      "Step: 6000 \t Cost: 0.5761464238166809\n",
      "Step: 6100 \t Cost: 0.558824896812439\n",
      "Step: 6200 \t Cost: 0.5396819710731506\n",
      "Step: 6300 \t Cost: 0.5183840394020081\n",
      "Step: 6400 \t Cost: 0.4942724406719208\n",
      "Step: 6500 \t Cost: 0.46656855940818787\n",
      "Step: 6600 \t Cost: 0.43501538038253784\n",
      "Step: 6700 \t Cost: 0.4003981351852417\n",
      "Step: 6800 \t Cost: 0.36432018876075745\n",
      "Step: 6900 \t Cost: 0.32857245206832886\n",
      "Step: 7000 \t Cost: 0.294644296169281\n",
      "Step: 7100 \t Cost: 0.2635124921798706\n",
      "Step: 7200 \t Cost: 0.23564468324184418\n",
      "Step: 7300 \t Cost: 0.21111518144607544\n",
      "Step: 7400 \t Cost: 0.18974828720092773\n",
      "Step: 7500 \t Cost: 0.17123860120773315\n",
      "Step: 7600 \t Cost: 0.15523456037044525\n",
      "Step: 7700 \t Cost: 0.14138828217983246\n",
      "Step: 7800 \t Cost: 0.129380464553833\n",
      "Step: 7900 \t Cost: 0.11893077194690704\n",
      "Step: 8000 \t Cost: 0.1097990870475769\n",
      "Step: 8100 \t Cost: 0.10178326815366745\n",
      "Step: 8200 \t Cost: 0.09471389651298523\n",
      "Step: 8300 \t Cost: 0.08845001459121704\n",
      "Step: 8400 \t Cost: 0.08287419378757477\n",
      "Step: 8500 \t Cost: 0.07788871228694916\n",
      "Step: 8600 \t Cost: 0.07341192662715912\n",
      "Step: 8700 \t Cost: 0.06937550008296967\n",
      "Step: 8800 \t Cost: 0.06572185456752777\n",
      "Step: 8900 \t Cost: 0.06240256130695343\n",
      "Step: 9000 \t Cost: 0.05937647446990013\n",
      "Step: 9100 \t Cost: 0.05660856515169144\n",
      "Step: 9200 \t Cost: 0.054068904370069504\n",
      "Step: 9300 \t Cost: 0.051731888204813004\n",
      "Step: 9400 \t Cost: 0.04957530274987221\n",
      "Step: 9500 \t Cost: 0.04758000373840332\n",
      "Step: 9600 \t Cost: 0.04572942480444908\n",
      "Step: 9700 \t Cost: 0.04400894418358803\n",
      "Step: 9800 \t Cost: 0.042405933141708374\n",
      "Step: 9900 \t Cost: 0.04090919345617294\n",
      "Step: 10000 \t Cost: 0.039508916437625885\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "    # forward\n",
    "    l1 = torch.add(torch.matmul(X, w1), b1)\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1, w2), b2)\n",
    "    Y_pred = sigmoid(l2)\n",
    "    \n",
    "    cost = -torch.mean(Y * torch.log(Y_pred) + (1 - Y) * torch.log(1 - Y_pred))\n",
    "    \n",
    "    \n",
    "    # Backprop (chain rule)\n",
    "    # Loss derivative\n",
    "    d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n",
    "    \n",
    "    # Layer 2\n",
    "    d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
    "    d_b2 = d_l2\n",
    "    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
    "    \n",
    "    # Layer 1\n",
    "    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
    "    d_l1 = d_a1 * sigmoid_prime(l1)\n",
    "    d_b1 = d_l1\n",
    "    d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
    "    \n",
    "    \n",
    "    # Weight update\n",
    "    w1 = w1 - learning_rate * d_w1\n",
    "    b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
    "    w2 = w2 - learning_rate * d_w2\n",
    "    b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
    "    \n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(\"Step: {} \\t Cost: {}\".format(step, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: XOR-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:20:55.383813Z",
     "start_time": "2019-05-14T03:20:51.111544Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 \t Cost: 0.7174476385116577\n",
      "Step: 100 \t Cost: 0.6931344270706177\n",
      "Step: 200 \t Cost: 0.6929052472114563\n",
      "Step: 300 \t Cost: 0.6922950744628906\n",
      "Step: 400 \t Cost: 0.688478946685791\n",
      "Step: 500 \t Cost: 0.6552730202674866\n",
      "Step: 600 \t Cost: 0.5433056354522705\n",
      "Step: 700 \t Cost: 0.3511308431625366\n",
      "Step: 800 \t Cost: 0.12058607488870621\n",
      "Step: 900 \t Cost: 0.05995449423789978\n",
      "Step: 1000 \t Cost: 0.03859895467758179\n",
      "Step: 1100 \t Cost: 0.028160858899354935\n",
      "Step: 1200 \t Cost: 0.022059038281440735\n",
      "Step: 1300 \t Cost: 0.01808222010731697\n",
      "Step: 1400 \t Cost: 0.015295113436877728\n",
      "Step: 1500 \t Cost: 0.013238020241260529\n",
      "Step: 1600 \t Cost: 0.011659765616059303\n",
      "Step: 1700 \t Cost: 0.010411925613880157\n",
      "Step: 1800 \t Cost: 0.009401406161487103\n",
      "Step: 1900 \t Cost: 0.008566854521632195\n",
      "Step: 2000 \t Cost: 0.007866371423006058\n",
      "Step: 2100 \t Cost: 0.0072702402248978615\n",
      "Step: 2200 \t Cost: 0.006756922230124474\n",
      "Step: 2300 \t Cost: 0.006310423836112022\n",
      "Step: 2400 \t Cost: 0.005918555893003941\n",
      "Step: 2500 \t Cost: 0.0055719343945384026\n",
      "Step: 2600 \t Cost: 0.005263215396553278\n",
      "Step: 2700 \t Cost: 0.004986492916941643\n",
      "Step: 2800 \t Cost: 0.004737098701298237\n",
      "Step: 2900 \t Cost: 0.004511239938437939\n",
      "Step: 3000 \t Cost: 0.004305681213736534\n",
      "Step: 3100 \t Cost: 0.004117838107049465\n",
      "Step: 3200 \t Cost: 0.003945544827729464\n",
      "Step: 3300 \t Cost: 0.0037869540974497795\n",
      "Step: 3400 \t Cost: 0.003640503389760852\n",
      "Step: 3500 \t Cost: 0.0035048716235905886\n",
      "Step: 3600 \t Cost: 0.003378842957317829\n",
      "Step: 3700 \t Cost: 0.0032614869996905327\n",
      "Step: 3800 \t Cost: 0.0031519634649157524\n",
      "Step: 3900 \t Cost: 0.0030494770035147667\n",
      "Step: 4000 \t Cost: 0.002953383605927229\n",
      "Step: 4100 \t Cost: 0.002863127738237381\n",
      "Step: 4200 \t Cost: 0.002778140362352133\n",
      "Step: 4300 \t Cost: 0.0026980764232575893\n",
      "Step: 4400 \t Cost: 0.0026223966851830482\n",
      "Step: 4500 \t Cost: 0.002550861332565546\n",
      "Step: 4600 \t Cost: 0.0024830056354403496\n",
      "Step: 4700 \t Cost: 0.0024187550880014896\n",
      "Step: 4800 \t Cost: 0.0023576603271067142\n",
      "Step: 4900 \t Cost: 0.00229955674149096\n",
      "Step: 5000 \t Cost: 0.002244249451905489\n",
      "Step: 5100 \t Cost: 0.0021914991084486246\n",
      "Step: 5200 \t Cost: 0.002141155768185854\n",
      "Step: 5300 \t Cost: 0.002093040384352207\n",
      "Step: 5400 \t Cost: 0.0020470176823437214\n",
      "Step: 5500 \t Cost: 0.00200301269069314\n",
      "Step: 5600 \t Cost: 0.0019608165603131056\n",
      "Step: 5700 \t Cost: 0.001920324400998652\n",
      "Step: 5800 \t Cost: 0.0018814909271895885\n",
      "Step: 5900 \t Cost: 0.0018441968131810427\n",
      "Step: 6000 \t Cost: 0.0018083371687680483\n",
      "Step: 6100 \t Cost: 0.001773777767084539\n",
      "Step: 6200 \t Cost: 0.0017405779799446464\n",
      "Step: 6300 \t Cost: 0.0017085584113374352\n",
      "Step: 6400 \t Cost: 0.0016776744741946459\n",
      "Step: 6500 \t Cost: 0.0016479108016937971\n",
      "Step: 6600 \t Cost: 0.0016191781032830477\n",
      "Step: 6700 \t Cost: 0.0015914164250716567\n",
      "Step: 6800 \t Cost: 0.0015645658131688833\n",
      "Step: 6900 \t Cost: 0.0015386263839900494\n",
      "Step: 7000 \t Cost: 0.001513508497737348\n",
      "Step: 7100 \t Cost: 0.0014891971368342638\n",
      "Step: 7200 \t Cost: 0.001465662382543087\n",
      "Step: 7300 \t Cost: 0.0014428446302190423\n",
      "Step: 7400 \t Cost: 0.0014207435306161642\n",
      "Step: 7500 \t Cost: 0.0013992995955049992\n",
      "Step: 7600 \t Cost: 0.0013784231850877404\n",
      "Step: 7700 \t Cost: 0.0013582336250692606\n",
      "Step: 7800 \t Cost: 0.0013385816710069776\n",
      "Step: 7900 \t Cost: 0.0013195120263844728\n",
      "Step: 8000 \t Cost: 0.001300979871302843\n",
      "Step: 8100 \t Cost: 0.001282925484701991\n",
      "Step: 8200 \t Cost: 0.0012654087040573359\n",
      "Step: 8300 \t Cost: 0.0012483547907322645\n",
      "Step: 8400 \t Cost: 0.001231703907251358\n",
      "Step: 8500 \t Cost: 0.0012155307922512293\n",
      "Step: 8600 \t Cost: 0.0011997756082564592\n",
      "Step: 8700 \t Cost: 0.0011843937681987882\n",
      "Step: 8800 \t Cost: 0.001169400056824088\n",
      "Step: 8900 \t Cost: 0.0011547794565558434\n",
      "Step: 9000 \t Cost: 0.001140517182648182\n",
      "Step: 9100 \t Cost: 0.0011266280198469758\n",
      "Step: 9200 \t Cost: 0.0011130673810839653\n",
      "Step: 9300 \t Cost: 0.001099834917113185\n",
      "Step: 9400 \t Cost: 0.0010868713725358248\n",
      "Step: 9500 \t Cost: 0.0010742361191660166\n",
      "Step: 9600 \t Cost: 0.001061869552358985\n",
      "Step: 9700 \t Cost: 0.001049786456860602\n",
      "Step: 9800 \t Cost: 0.0010379720479249954\n",
      "Step: 9900 \t Cost: 0.0010264711454510689\n",
      "Step: 10000 \t Cost: 0.0010151790920644999\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# nn layers\n",
    "linear1 = nn.Linear(2, 2, bias=True)\n",
    "linear2 = nn.Linear(2, 1, bias=True)\n",
    "sigmoid = nn.Sigmoid()\n",
    "model = nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    \n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(\"Step: {} \\t Cost: {}\".format(step, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: XOR-nn Wide&Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:22:46.545682Z",
     "start_time": "2019-05-14T03:22:39.321761Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 \t Cost: 0.6932129859924316\n",
      "Step: 100 \t Cost: 0.6931434273719788\n",
      "Step: 200 \t Cost: 0.6931430697441101\n",
      "Step: 300 \t Cost: 0.6931426525115967\n",
      "Step: 400 \t Cost: 0.693142294883728\n",
      "Step: 500 \t Cost: 0.6931418180465698\n",
      "Step: 600 \t Cost: 0.6931414604187012\n",
      "Step: 700 \t Cost: 0.6931411027908325\n",
      "Step: 800 \t Cost: 0.6931406855583191\n",
      "Step: 900 \t Cost: 0.6931402683258057\n",
      "Step: 1000 \t Cost: 0.6931398510932922\n",
      "Step: 1100 \t Cost: 0.6931394338607788\n",
      "Step: 1200 \t Cost: 0.6931390166282654\n",
      "Step: 1300 \t Cost: 0.693138599395752\n",
      "Step: 1400 \t Cost: 0.6931381225585938\n",
      "Step: 1500 \t Cost: 0.6931377053260803\n",
      "Step: 1600 \t Cost: 0.6931371688842773\n",
      "Step: 1700 \t Cost: 0.6931366920471191\n",
      "Step: 1800 \t Cost: 0.6931362152099609\n",
      "Step: 1900 \t Cost: 0.693135678768158\n",
      "Step: 2000 \t Cost: 0.693135142326355\n",
      "Step: 2100 \t Cost: 0.6931344270706177\n",
      "Step: 2200 \t Cost: 0.6931338310241699\n",
      "Step: 2300 \t Cost: 0.6931332349777222\n",
      "Step: 2400 \t Cost: 0.6931325793266296\n",
      "Step: 2500 \t Cost: 0.6931318640708923\n",
      "Step: 2600 \t Cost: 0.6931310892105103\n",
      "Step: 2700 \t Cost: 0.6931302547454834\n",
      "Step: 2800 \t Cost: 0.6931294202804565\n",
      "Step: 2900 \t Cost: 0.6931285858154297\n",
      "Step: 3000 \t Cost: 0.6931276321411133\n",
      "Step: 3100 \t Cost: 0.6931265592575073\n",
      "Step: 3200 \t Cost: 0.6931255459785461\n",
      "Step: 3300 \t Cost: 0.6931243538856506\n",
      "Step: 3400 \t Cost: 0.6931231021881104\n",
      "Step: 3500 \t Cost: 0.6931217908859253\n",
      "Step: 3600 \t Cost: 0.6931203007698059\n",
      "Step: 3700 \t Cost: 0.6931187510490417\n",
      "Step: 3800 \t Cost: 0.6931170225143433\n",
      "Step: 3900 \t Cost: 0.6931151747703552\n",
      "Step: 4000 \t Cost: 0.6931131482124329\n",
      "Step: 4100 \t Cost: 0.6931109428405762\n",
      "Step: 4200 \t Cost: 0.6931083798408508\n",
      "Step: 4300 \t Cost: 0.6931057572364807\n",
      "Step: 4400 \t Cost: 0.6931027173995972\n",
      "Step: 4500 \t Cost: 0.6930994391441345\n",
      "Step: 4600 \t Cost: 0.6930957436561584\n",
      "Step: 4700 \t Cost: 0.6930914521217346\n",
      "Step: 4800 \t Cost: 0.6930866837501526\n",
      "Step: 4900 \t Cost: 0.6930813193321228\n",
      "Step: 5000 \t Cost: 0.6930750608444214\n",
      "Step: 5100 \t Cost: 0.6930680274963379\n",
      "Step: 5200 \t Cost: 0.6930598020553589\n",
      "Step: 5300 \t Cost: 0.6930502653121948\n",
      "Step: 5400 \t Cost: 0.6930389404296875\n",
      "Step: 5500 \t Cost: 0.6930254697799683\n",
      "Step: 5600 \t Cost: 0.6930091977119446\n",
      "Step: 5700 \t Cost: 0.6929893493652344\n",
      "Step: 5800 \t Cost: 0.6929647326469421\n",
      "Step: 5900 \t Cost: 0.6929339170455933\n",
      "Step: 6000 \t Cost: 0.6928942799568176\n",
      "Step: 6100 \t Cost: 0.6928421854972839\n",
      "Step: 6200 \t Cost: 0.6927719116210938\n",
      "Step: 6300 \t Cost: 0.692673921585083\n",
      "Step: 6400 \t Cost: 0.6925314664840698\n",
      "Step: 6500 \t Cost: 0.6923131942749023\n",
      "Step: 6600 \t Cost: 0.691954493522644\n",
      "Step: 6700 \t Cost: 0.6913036704063416\n",
      "Step: 6800 \t Cost: 0.6899322271347046\n",
      "Step: 6900 \t Cost: 0.6861987113952637\n",
      "Step: 7000 \t Cost: 0.6692994832992554\n",
      "Step: 7100 \t Cost: 0.513489842414856\n",
      "Step: 7200 \t Cost: 0.07922688126564026\n",
      "Step: 7300 \t Cost: 0.012150105088949203\n",
      "Step: 7400 \t Cost: 0.005725922994315624\n",
      "Step: 7500 \t Cost: 0.0036033205687999725\n",
      "Step: 7600 \t Cost: 0.0025831740349531174\n",
      "Step: 7700 \t Cost: 0.001993633806705475\n",
      "Step: 7800 \t Cost: 0.001613230095244944\n",
      "Step: 7900 \t Cost: 0.001349040074273944\n",
      "Step: 8000 \t Cost: 0.0011557365069165826\n",
      "Step: 8100 \t Cost: 0.0010085462126880884\n",
      "Step: 8200 \t Cost: 0.0008930856129154563\n",
      "Step: 8300 \t Cost: 0.0008001870010048151\n",
      "Step: 8400 \t Cost: 0.0007239984697662294\n",
      "Step: 8500 \t Cost: 0.0006604155641980469\n",
      "Step: 8600 \t Cost: 0.0006066476344130933\n",
      "Step: 8700 \t Cost: 0.0005605759797617793\n",
      "Step: 8800 \t Cost: 0.00052070856327191\n",
      "Step: 8900 \t Cost: 0.00048588181380182505\n",
      "Step: 9000 \t Cost: 0.00045526030589826405\n",
      "Step: 9100 \t Cost: 0.0004280982830096036\n",
      "Step: 9200 \t Cost: 0.0004038439365103841\n",
      "Step: 9300 \t Cost: 0.0003820498241111636\n",
      "Step: 9400 \t Cost: 0.00036244746297597885\n",
      "Step: 9500 \t Cost: 0.0003446193295530975\n",
      "Step: 9600 \t Cost: 0.00032840133644640446\n",
      "Step: 9700 \t Cost: 0.00031356990803033113\n",
      "Step: 9800 \t Cost: 0.0002999758580699563\n",
      "Step: 9900 \t Cost: 0.0002874551573768258\n",
      "Step: 10000 \t Cost: 0.0002759034978225827\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# nn layers\n",
    "linear1 = nn.Linear(2, 10, bias=True)\n",
    "linear2 = nn.Linear(10, 10, bias=True)\n",
    "linear3 = nn.Linear(10, 10, bias=True)\n",
    "linear4 = nn.Linear(10, 1, bias=True)\n",
    "sigmoid = nn.Sigmoid()\n",
    "model = nn.Sequential(linear1, sigmoid,\n",
    "                      linear2, sigmoid,\n",
    "                      linear3, sigmoid,\n",
    "                      linear4, sigmoid).to(device)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    \n",
    "    # cost/loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(\"Step: {} \\t Cost: {}\".format(step, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "별도의 내용이 없어서 생략 (ReLU와 Optimizer에 대해서 소개)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: mnist_nn_xavier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan_in} + \\text{fan_out}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T04:57:50.255136Z",
     "start_time": "2019-05-14T04:57:50.252551Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.init import _calculate_fan_in_and_fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T04:57:52.968270Z",
     "start_time": "2019-05-14T04:57:52.963014Z"
    }
   },
   "outputs": [],
   "source": [
    "def xavier_uniform_(tensor, gain=1):\n",
    "    '''\n",
    "    Also known as glorot initialization.\n",
    "    \n",
    "    Args:\n",
    "        tensor: a n-dimensional `torch.Tensor`\n",
    "        gain: an optional scaling factor\n",
    "        \n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n",
    "    '''\n",
    "    # Check: https://github.com/pytorch/pytorch/blob/8e9692df2787b64f879e83db617745b810bd7ef2/torch/nn/init.py#L209\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
    "    std = gain * math.sqrt(2.0/(fan_in + fan_out))\n",
    "    a = math.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        return tensor.uniform_(-a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따로 추가할 내용이 없어 생략\n",
    "- 요약 하자면, torch.nn.Dropout(p=drop_prob)를 쓰는 이야기\n",
    "- layer → activation → dropout순으로 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Vanishing / Exploding\n",
    "- Internal Covariate Shift\n",
    "    - Covariate Shift\n",
    "        - Training과 Test셋의 분포에 차이가 있을 수 있으며, 이로 인해 문제가 발생할 수 있음\n",
    "        - `입력과 출력의 분포가 다르다`라고도 생각할 수 있음\n",
    "    - Internal Covariate Shift\n",
    "        - 입력과 출력이 아닌 `Layer`간의 차이 때문에 Covariate Shift가 발생할 수 있다.\n",
    "        - 이는 학습이 진행될 수록 각 Layer의 parameter의 분포가 다를 수 있기 때문에 생기는 문제이다.\n",
    "- Batch Normalization\n",
    "    - Layer마다 Normalization을 해서 변형된 분포가 나오지 않도록 하고자 하는 과정\n",
    "    - 주의: 입력에 대한 분포를 Normalize하는 과정이다.\n",
    "    - Normalize가 끝난 결과에 Gamma를 곱해주고 Beta를 더해주는 연산이 Batch Normalization이다.\n",
    "        - Gamma와 Beta는 Trainable이다.\n",
    "    - Sample mean/variance: Batch-size별 입력에 대한 mean/variancec\n",
    "    - Running mean/variance: 모든 batch에 대한 sample mean/variance를 평균한 value\n",
    "    - Layer → BN → Activation → ... 순으로 사용하는 것이 일반적인 사용법이다.\n",
    "    - PyTorch에서 BatchNorm이나 Dropout을 사용할 시에는 `model.train()`을 이용해 학습 모드를 꼭 지정하도록 하자.\n",
    "        - `model.eval()`은 Validation/Evaluation시 사용하도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: MNIST_batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T07:02:02.351742Z",
     "start_time": "2019-05-14T07:02:02.328297Z"
    }
   },
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 32, bias=True)\n",
    "linear2 = torch.nn.Linear(32, 32, bias=True)\n",
    "linear3 = torch.nn.Linear(32, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "bn1 = torch.nn.BatchNorm1d(32)    # 1D Batch-normalization\n",
    "bn2 = torch.nn.BatchNorm1d(32)\n",
    "\n",
    "nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n",
    "nn_linear2 = torch.nn.Linear(32, 32, bias=True)\n",
    "nn_linear3 = torch.nn.Linear(32, 10, bias=True)\n",
    "\n",
    "# model\n",
    "bn_model = torch.nn.Sequential(linear1, bn1, relu,\n",
    "                               linear2, bn2, relu,\n",
    "                               linear3).to(device)\n",
    "nn_model = torch.nn.Sequential(nn_linear1, relu,\n",
    "                               nn_linear2, relu,\n",
    "                               nn_linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
